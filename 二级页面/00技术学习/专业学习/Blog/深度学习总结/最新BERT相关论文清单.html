<html>
<head>
  <title>最新BERT相关论文清单汇总</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/600753 (zh-CN, DDL); Windows/10.0.0 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="8413"/>

<div>
<span><div style="padding:8px 0px;margin:8px auto;max-width:700px;"><div style="margin:0;padding:0;background-color:#fff"><div style="margin:0;padding:0"><div style="margin:0;padding:0"><h2 style="margin: 0px 0px 14px; padding: 0px; font-size: 22px;"><span style="margin:0px;"><span style="font-size: 22px; font-weight: 400; line-height: 1.4;">最新BERT相关论文清单汇总</span></span></h2><div style="margin: 0px 0px 22px; padding: 0px; font-size: 0px; word-wrap: break-word; word-break: break-all;"><br/></div><div style="margin: 0px; padding: 0px; overflow: hidden; font-size: 17px; word-wrap: break-word; text-align: justify; position: relative; z-index: 0;"><div style="margin: 0px; padding: 0px; clear: both; min-height: 1em; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"><span style="margin: 0px; padding: 0px; font-size: 15px; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important; color: rgb(51, 51, 51);">【导读】词向量模型作为NLP基础设施，对下游任务影响巨大。本文为大家推荐一份BERT相关论文清单，帮助初学者快速入手前沿词向量相关研究工作。</span></div><div style="margin: 0px; padding: 0px; clear: both; min-height: 1em; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"><span style="margin:0;padding:0;max-width:100% !important;box-sizing:border-box !important;-webkit-box-sizing:border-box !important;word-wrap:break-word !important;font-size:15px"><br style="margin:0;padding:0;max-width:100% !important;box-sizing:border-box !important;-webkit-box-sizing:border-box !important;word-wrap:break-word !important"/></span></div><div style="margin: 0px; padding: 0px; clear: both; min-height: 1em; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"><strong style="margin:0;padding:0;max-width:100% !important;box-sizing:border-box !important;-webkit-box-sizing:border-box !important;word-wrap:break-word !important"><span style="margin: 0px; padding: 0px; font-size: 15px; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important; color: rgb(51, 51, 51); font-weight: bold;">原文链接：</span></strong></div><div style="margin: 0px; padding: 0px; clear: both; min-height: 1em; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"><span style="margin: 0px; padding: 0px; font-size: 15px; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important; color: rgb(0, 82, 255);"><a href="https://github.com/tomohideshibata/BERT-related-papers">https://github.com/tomohideshibata/BERT-related-papers</a></span></div><div style="box-sizing: border-box; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">This is a list of BERT-related papers. Any feedback is welcome.</span></div><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Table of Contents</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#downstream-task" style="box-sizing: border-box; color: rgb(3, 102, 214);">Downstream task</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#generation" style="box-sizing: border-box; color: rgb(3, 102, 214);">Generation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#modification-multi-task-masking-strategy-etc" style="box-sizing: border-box; color: rgb(3, 102, 214);">Modification (multi-task, masking strategy, etc.)</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#probe" style="box-sizing: border-box; color: rgb(3, 102, 214);">Probe</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#inside-bert" style="box-sizing: border-box; color: rgb(3, 102, 214);">Inside BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#multi-lingual" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-lingual</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#domain-specific" style="box-sizing: border-box; color: rgb(3, 102, 214);">Domain specific</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#multi-modal" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-modal</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#model-compression" style="box-sizing: border-box; color: rgb(3, 102, 214);">Model compression</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://github.com/tomohideshibata/BERT-related-papers#misc" style="box-sizing: border-box; color: rgb(3, 102, 214);">Misc.</a></div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Downstream task</span></h2><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">QA, MC, Dialogue</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1901.08634" style="box-sizing: border-box; color: rgb(3, 102, 214);">A BERT Baseline for the Natural Questions</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.13453" style="box-sizing: border-box; color: rgb(3, 102, 214);">MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05514" style="box-sizing: border-box; color: rgb(3, 102, 214);">A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1812.03593" style="box-sizing: border-box; color: rgb(3, 102, 214);">SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1902.01718" style="box-sizing: border-box; color: rgb(3, 102, 214);">End-to-End Open-Domain Question Answering with BERTserini</a> (NAALC2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.00300" style="box-sizing: border-box; color: rgb(3, 102, 214);">Latent Retrieval for Weakly Supervised Open Domain Question Answering</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.08167" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.06045" style="box-sizing: border-box; color: rgb(3, 102, 214);">Learning to Ask Unanswerable Questions for Machine Reading Comprehension</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.04980" style="box-sizing: border-box; color: rgb(3, 102, 214);">Unsupervised Question Answering by Cloze Translation</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.04942" style="box-sizing: border-box; color: rgb(3, 102, 214);">Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1226/" style="box-sizing: border-box; color: rgb(3, 102, 214);">Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.04530" style="box-sizing: border-box; color: rgb(3, 102, 214);">Incorporating Relation Knowledge into Commonsense Reading Comprehension with Multi-task Learning</a> (CIKM2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05147" style="box-sizing: border-box; color: rgb(3, 102, 214);">SG-Net: Syntax-Guided Machine Reading Comprehension</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.12848" style="box-sizing: border-box; color: rgb(3, 102, 214);">A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension</a>(ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05117" style="box-sizing: border-box; color: rgb(3, 102, 214);">FlowDelta: Modeling Flow Information Gain in Reasoning for Conversational Machine Comprehension</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.05412" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT with History Answer Embedding for Conversational Question Answering</a> (SIGIR2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.00059" style="box-sizing: border-box; color: rgb(3, 102, 214);">GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension</a>(ICML2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.01519" style="box-sizing: border-box; color: rgb(3, 102, 214);">Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian</a> (RANLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.00361" style="box-sizing: border-box; color: rgb(3, 102, 214);">Cross-Lingual Machine Reading Comprehension</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.09587" style="box-sizing: border-box; color: rgb(3, 102, 214);">Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.00109" style="box-sizing: border-box; color: rgb(3, 102, 214);">Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1902.10909" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT for Joint Intent Classification and Slot Filling</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.02884" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based Model</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.03040" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer</a>(Interspeech2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.01946" style="box-sizing: border-box; color: rgb(3, 102, 214);">Dialog State Tracking: A Neural Reading Comprehension Approach</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.04812" style="box-sizing: border-box; color: rgb(3, 102, 214);">Domain Adaptive Training BERT for Response Selection</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Analysis</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1908.04755" style="box-sizing: border-box; color: rgb(3, 102, 214);">Fine-grained Information Status Classification Using Discourse Context-Aware Self-Attention</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.07245" style="box-sizing: border-box; color: rgb(3, 102, 214);">GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1910.00194" style="box-sizing: border-box; color: rgb(3, 102, 214);">Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.08358" style="box-sizing: border-box; color: rgb(3, 102, 214);">Using BERT for Word Sense Disambiguation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1903.09588" style="box-sizing: border-box; color: rgb(3, 102, 214);">Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence</a> (NAACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.02232" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a> (NAACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.11860" style="box-sizing: border-box; color: rgb(3, 102, 214);">Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.09642" style="box-sizing: border-box; color: rgb(3, 102, 214);">An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.08039" style="box-sizing: border-box; color: rgb(3, 102, 214);">&quot;Mask and Infill&quot; : Applying Masked Language Model to Sentiment Transfer</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.03750" style="box-sizing: border-box; color: rgb(3, 102, 214);">Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1901.05287" style="box-sizing: border-box; color: rgb(3, 102, 214);">Assessing BERT’s Syntactic Abilities</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.09892" style="box-sizing: border-box; color: rgb(3, 102, 214);">Does BERT agree? Evaluating knowledge of structure dependence through agreement relations</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.05255" style="box-sizing: border-box; color: rgb(3, 102, 214);">Simple BERT Models for Relation Extraction and Semantic Role Labeling</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.06226" style="box-sizing: border-box; color: rgb(3, 102, 214);">A Simple BERT-Based Approach for Lexical Simplification</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4426/" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-headed Architecture Based on BERT for Grammatical Errors Correction</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.03193" style="box-sizing: border-box; color: rgb(3, 102, 214);">KG-BERT: BERT for Knowledge Graph Completion</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.01066" style="box-sizing: border-box; color: rgb(3, 102, 214);">Language Models as Knowledge Bases?</a> (EMNLP2019) [<a href="https://github.com/facebookresearch/LAMA" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.04181" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT-Based Arabic Social Media Author Profiling</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Word segmentation / parsing / NER</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1909.09292" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT Meets Chinese Word Segmentation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1903.04190" style="box-sizing: border-box; color: rgb(3, 102, 214);">Toward Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.04943" style="box-sizing: border-box; color: rgb(3, 102, 214);">Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.07448" style="box-sizing: border-box; color: rgb(3, 102, 214);">Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.00204" style="box-sizing: border-box; color: rgb(3, 102, 214);">NEZHA: Neural Contextualized Representation for Chinese Language Understanding</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.07397" style="box-sizing: border-box; color: rgb(3, 102, 214);">Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing -- A Tale of Two Parsers Revisited</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.06775" style="box-sizing: border-box; color: rgb(3, 102, 214);">Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.10649" style="box-sizing: border-box; color: rgb(3, 102, 214);">Portuguese Named Entity Recognition using BERT-CRF</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Pronoun/coreference resolution</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1906.01161" style="box-sizing: border-box; color: rgb(3, 102, 214);">Resolving Gendered Ambiguous Pronouns with BERT</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.01780" style="box-sizing: border-box; color: rgb(3, 102, 214);">Anonymized BERT: An Augmentation Approach to the Gendered Pronoun Resolution Challenge</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.03695" style="box-sizing: border-box; color: rgb(3, 102, 214);">Gendered Pronoun Resolution using BERT and an extractive question answering formulation</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.00308" style="box-sizing: border-box; color: rgb(3, 102, 214);">MSnet: A BERT-based Network for Gendered Pronoun Resolution</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-3815/" style="box-sizing: border-box; color: rgb(3, 102, 214);">Fill the GAP: Exploiting BERT for Pronoun Resolution</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.08868" style="box-sizing: border-box; color: rgb(3, 102, 214);">Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</a>(ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-3811/" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT Masked Language Modeling for Co-reference Resolution</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.09091" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT for Coreference Resolution: Baselines and Analysis</a> (EMNLP2019) [<a href="https://github.com/mandarjoshi90/coref" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.08025" style="box-sizing: border-box; color: rgb(3, 102, 214);">WikiCREM: A Large Unsupervised Corpus for Coreference Resolution</a> (EMNLP2019)</div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Relation extraction</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1906.03158" style="box-sizing: border-box; color: rgb(3, 102, 214);">Matching the Blanks: Distributional Similarity for Relation Learning</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05908" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction</a> (NLPCC2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.08284" style="box-sizing: border-box; color: rgb(3, 102, 214);">Enriching Pre-trained Language Model with Entity Information for Relation Classification</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.07755" style="box-sizing: border-box; color: rgb(3, 102, 214);">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.11898" style="box-sizing: border-box; color: rgb(3, 102, 214);">Fine-tune Bert for DocRED with Two-step Process</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.07721" style="box-sizing: border-box; color: rgb(3, 102, 214);">Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Text classification</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1905.05583" style="box-sizing: border-box; color: rgb(3, 102, 214);">How to Fine-Tune BERT for Text Classification?</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.02331" style="box-sizing: border-box; color: rgb(3, 102, 214);">X-BERT: eXtreme Multi-label Text Classification with BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.08398" style="box-sizing: border-box; color: rgb(3, 102, 214);">DocBERT: BERT for Document Classification</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.08402" style="box-sizing: border-box; color: rgb(3, 102, 214);">Enriching BERT with Knowledge Graph Embeddings for Document Classification</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.09821" style="box-sizing: border-box; color: rgb(3, 102, 214);">Classification and Clustering of Arguments with Contextualized Word Embeddings</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1812.06705" style="box-sizing: border-box; color: rgb(3, 102, 214);">Conditional BERT Contextual Augmentation</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">WSC, WNLI, NLI</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1904.09705" style="box-sizing: border-box; color: rgb(3, 102, 214);">Exploring Unsupervised Pretraining and Sentence Structure Modelling for Winograd Schema Challenge</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.06290" style="box-sizing: border-box; color: rgb(3, 102, 214);">A Surprisingly Robust Trick for the Winograd Schema Challenge</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.08217" style="box-sizing: border-box; color: rgb(3, 102, 214);">Improving Natural Language Inference with a Pretrained Parser</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Commonsense</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1905.07830" style="box-sizing: border-box; color: rgb(3, 102, 214);">HellaSwag: Can a Machine Really Finish Your Sentence?</a> (ACL2019) [<a href="https://rowanzellers.com/hellaswag/" style="box-sizing: border-box; color: rgb(3, 102, 214);">website</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.02361" style="box-sizing: border-box; color: rgb(3, 102, 214);">Explain Yourself! Leveraging Language Models for Commonsense Reasoning</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.06725" style="box-sizing: border-box; color: rgb(3, 102, 214);">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.02339" style="box-sizing: border-box; color: rgb(3, 102, 214);">Informing Unsupervised Pretraining with External Linguistic Knowledge</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.03415" style="box-sizing: border-box; color: rgb(3, 102, 214);">Commonsense Knowledge + BERT for Level 2 Reading Comprehension Ability Test</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Extractive summarization</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1905.06566" style="box-sizing: border-box; color: rgb(3, 102, 214);">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.03223" style="box-sizing: border-box; color: rgb(3, 102, 214);">Deleter: Leveraging BERT to Perform Unsupervised Successive Text Compression</a></div></li></ul><h3 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-size: 1.25em; font-weight: bold; line-height: 1.25; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">IR</span></h3><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1901.04085" style="box-sizing: border-box; color: rgb(3, 102, 214);">Passage Re-ranking with BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.01758" style="box-sizing: border-box; color: rgb(3, 102, 214);">Investigating the Successes and Failures of BERT for Passage Re-Ranking</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.07531" style="box-sizing: border-box; color: rgb(3, 102, 214);">Understanding the Behaviors of BERT in Ranking</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.08375" style="box-sizing: border-box; color: rgb(3, 102, 214);">Document Expansion by Query Prediction</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.07094" style="box-sizing: border-box; color: rgb(3, 102, 214);">CEDR: Contextualized Embeddings for Document Ranking</a> (SIGIR2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.09217" style="box-sizing: border-box; color: rgb(3, 102, 214);">Deeper Text Understanding for IR with Contextual Neural Language Modeling</a> (SIGIR2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.02851" style="box-sizing: border-box; color: rgb(3, 102, 214);">FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance</a> (SIGIR2019)</div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Generation</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1902.04094" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</a> (NAACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1902.09243" style="box-sizing: border-box; color: rgb(3, 102, 214);">Pretraining-Based Natural Language Generation for Text Summarization</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.08345" style="box-sizing: border-box; color: rgb(3, 102, 214);">Text Summarization with Pretrained Encoders</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.10599" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-stage Pretraining for Abstractive Summarization</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.02450" style="box-sizing: border-box; color: rgb(3, 102, 214);">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a> (ICML2019) [<a href="https://github.com/microsoft/MASS" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>], [<a href="https://github.com/microsoft/MASS/tree/master/MASS-fairseq" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.03197" style="box-sizing: border-box; color: rgb(3, 102, 214);">Unified Language Model Pre-training for Natural Language Understanding and Generation</a> (NeurIPS2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05672" style="box-sizing: border-box; color: rgb(3, 102, 214);">Towards Making the Most of BERT in Neural Machine Translation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.07688" style="box-sizing: border-box; color: rgb(3, 102, 214);">Improving Neural Machine Translation with Pre-trained Representation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.12744" style="box-sizing: border-box; color: rgb(3, 102, 214);">On the use of BERT for Neural Machine Translation</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.09324" style="box-sizing: border-box; color: rgb(3, 102, 214);">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</a> (EMNLP2019)</div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Modification (multi-task, masking strategy, etc.)</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1901.11504" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multi-Task Deep Neural Networks for Natural Language Understanding</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1902.02671" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning</a> (ICML2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.09286" style="box-sizing: border-box; color: rgb(3, 102, 214);">Unifying Question Answering and Text Classification via Span Extraction</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.07129" style="box-sizing: border-box; color: rgb(3, 102, 214);">ERNIE: Enhanced Language Representation with Informative Entities</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.09223" style="box-sizing: border-box; color: rgb(3, 102, 214);">ERNIE: Enhanced Representation through Knowledge Integration</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.12412" style="box-sizing: border-box; color: rgb(3, 102, 214);">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.10529" style="box-sizing: border-box; color: rgb(3, 102, 214);">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a> [<a href="https://github.com/facebookresearch/SpanBERT" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.11692" style="box-sizing: border-box; color: rgb(3, 102, 214);">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> [<a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.11942" style="box-sizing: border-box; color: rgb(3, 102, 214);">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.01604" style="box-sizing: border-box; color: rgb(3, 102, 214);">KERMIT: Generative Insertion-Based Modeling for Sequences</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1710.04334" style="box-sizing: border-box; color: rgb(3, 102, 214);">DisSent: Sentence Representation Learning from Explicit Discourse Relations</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.04577" style="box-sizing: border-box; color: rgb(3, 102, 214);">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05646" style="box-sizing: border-box; color: rgb(3, 102, 214);">SenseBERT: Driving Some Sense into BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.02209" style="box-sizing: border-box; color: rgb(3, 102, 214);">Semantics-aware BERT for Language Understanding</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.07606" style="box-sizing: border-box; color: rgb(3, 102, 214);">K-BERT: Enabling Language Representation with Knowledge Graph</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.04164" style="box-sizing: border-box; color: rgb(3, 102, 214);">Knowledge Enhanced Contextual Word Representations</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.10084" style="box-sizing: border-box; color: rgb(3, 102, 214);">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.03405" style="box-sizing: border-box; color: rgb(3, 102, 214);">Symmetric Regularization based BERT for Pair-wise Semantic Reasoning</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.00931" style="box-sizing: border-box; color: rgb(3, 102, 214);">Transfer Fine-Tuning: A BERT Case Study</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.12440" style="box-sizing: border-box; color: rgb(3, 102, 214);">Improving Pre-Trained Multilingual Models with Vocabulary Expansion</a> (CoNLL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://openreview.net/forum?id=r1xMH1BtvB" style="box-sizing: border-box; color: rgb(3, 102, 214);">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Probe</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://aclweb.org/anthology/papers/N/N19/N19-1419/" style="box-sizing: border-box; color: rgb(3, 102, 214);">A Structural Probe for Finding Syntax in Word Representations</a> (NAACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1903.08855" style="box-sizing: border-box; color: rgb(3, 102, 214);">Linguistic Knowledge and Transferability of Contextual Representations</a> (NAACL2019) [<a href="https://github.com/nelson-liu/contextual-repr-analysis" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.11544" style="box-sizing: border-box; color: rgb(3, 102, 214);">Probing What Different NLP Tasks Teach Machines about Function Word Comprehension</a> (*SEM2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.05950" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT Rediscovers the Classical NLP Pipeline</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.07355" style="box-sizing: border-box; color: rgb(3, 102, 214);">Probing Neural Network Comprehension of Natural Language Arguments</a> (ACL2019)</div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Inside BERT</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://hal.inria.fr/hal-02131630/document" style="box-sizing: border-box; color: rgb(3, 102, 214);">What does BERT learn about the structure of language?</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.01698" style="box-sizing: border-box; color: rgb(3, 102, 214);">Open Sesame: Getting Inside BERT's Linguistic Knowledge</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.04284" style="box-sizing: border-box; color: rgb(3, 102, 214);">Analyzing the Structure of Attention in a Transformer Language Model</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.04341" style="box-sizing: border-box; color: rgb(3, 102, 214);">What Does BERT Look At? An Analysis of BERT's Attention</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.01539" style="box-sizing: border-box; color: rgb(3, 102, 214);">Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains</a>(ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.11511" style="box-sizing: border-box; color: rgb(3, 102, 214);">Inducing Syntactic Trees from BERT Representations</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.05714" style="box-sizing: border-box; color: rgb(3, 102, 214);">A Multiscale Visualization of Attention in the Transformer Model</a> (ACL2019 Demo)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.02715" style="box-sizing: border-box; color: rgb(3, 102, 214);">Visualizing and Measuring the Geometry of BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.00512" style="box-sizing: border-box; color: rgb(3, 102, 214);">How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1905.10650" style="box-sizing: border-box; color: rgb(3, 102, 214);">Are Sixteen Heads Really Better than One?</a> (NeurIPS2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.04211" style="box-sizing: border-box; color: rgb(3, 102, 214);">On the Validity of Self-Attention as Explanation in Transformer Models</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05620" style="box-sizing: border-box; color: rgb(3, 102, 214);">Visualizing and Understanding the Effectiveness of BERT</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.11218" style="box-sizing: border-box; color: rgb(3, 102, 214);">Attention Interpretability Across NLP Tasks</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.08593" style="box-sizing: border-box; color: rgb(3, 102, 214);">Revealing the Dark Secrets of BERT</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.02597" style="box-sizing: border-box; color: rgb(3, 102, 214);">Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.01380" style="box-sizing: border-box; color: rgb(3, 102, 214);">The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.07940" style="box-sizing: border-box; color: rgb(3, 102, 214);">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.04925" style="box-sizing: border-box; color: rgb(3, 102, 214);">How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</a> (CIKM2019)</div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Multi-lingual</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1812.11760" style="box-sizing: border-box; color: rgb(3, 102, 214);">Multilingual Constituency Parsing with Self-Attention and Pre-Training</a> (ACL2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1901.07291" style="box-sizing: border-box; color: rgb(3, 102, 214);">Cross-lingual Language Model Pretraining</a> (NeurIPS2019) [<a href="https://github.com/facebookresearch/XLM" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.02099" style="box-sizing: border-box; color: rgb(3, 102, 214);">75 Languages, 1 Model: Parsing Universal Dependencies Universally</a> (EMNLP2019) [<a href="https://github.com/hyperparticle/udify" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.09077" style="box-sizing: border-box; color: rgb(3, 102, 214);">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.01502" style="box-sizing: border-box; color: rgb(3, 102, 214);">How multilingual is Multilingual BERT?</a> (ACL2019)</div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Domain specific</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1901.08746" style="box-sizing: border-box; color: rgb(3, 102, 214);">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.05474" style="box-sizing: border-box; color: rgb(3, 102, 214);">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.03548" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERT-based Ranking for Biomedical Entity Normalization</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.06146" style="box-sizing: border-box; color: rgb(3, 102, 214);">PubMedQA: A Dataset for Biomedical Research Question Answering</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.08229" style="box-sizing: border-box; color: rgb(3, 102, 214);">Pre-trained Language Model for Biomedical Question Answering</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.05342" style="box-sizing: border-box; color: rgb(3, 102, 214);">ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.03323" style="box-sizing: border-box; color: rgb(3, 102, 214);">Publicly Available Clinical BERT Embeddings</a> (NAACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1903.10676" style="box-sizing: border-box; color: rgb(3, 102, 214);">SciBERT: Pretrained Contextualized Embeddings for Scientific Text</a> [<a href="https://github.com/allenai/scibert" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.02124" style="box-sizing: border-box; color: rgb(3, 102, 214);">PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model</a></div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Multi-modal</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1904.01766" style="box-sizing: border-box; color: rgb(3, 102, 214);">VideoBERT: A Joint Model for Video and Language Representation Learning</a> (ICCV2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.02265" style="box-sizing: border-box; color: rgb(3, 102, 214);">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a> (NeurIPS2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.03557" style="box-sizing: border-box; color: rgb(3, 102, 214);">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.02940" style="box-sizing: border-box; color: rgb(3, 102, 214);">Selfie: Self-supervised Pretraining for Image Embedding</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1906.05743" style="box-sizing: border-box; color: rgb(3, 102, 214);">Contrastive Bidirectional Transformer for Temporal Representation Learning</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05787" style="box-sizing: border-box; color: rgb(3, 102, 214);">M-BERT: Injecting Multimodal Information in the BERT Structure</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.07490" style="box-sizing: border-box; color: rgb(3, 102, 214);">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.05054" style="box-sizing: border-box; color: rgb(3, 102, 214);">Fusion of Detected Objects in Text for Visual Question Answering</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.08530" style="box-sizing: border-box; color: rgb(3, 102, 214);">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.06066" style="box-sizing: border-box; color: rgb(3, 102, 214);">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Model compression</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1903.12136" style="box-sizing: border-box; color: rgb(3, 102, 214);">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1908.09355" style="box-sizing: border-box; color: rgb(3, 102, 214);">Patient Knowledge Distillation for BERT Model Compression</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.00100" style="box-sizing: border-box; color: rgb(3, 102, 214);">Small and Practical BERT Models for Sequence Labeling</a> (EMNLP2019)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.10351" style="box-sizing: border-box; color: rgb(3, 102, 214);">TinyBERT: Distilling BERT for Natural Language Understanding</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1910.01108" style="box-sizing: border-box; color: rgb(3, 102, 214);">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> (NeurIPS2019 WS) [<a href="https://github.com/huggingface/transformers/tree/master/examples/distillation" style="box-sizing: border-box; color: rgb(3, 102, 214);">github</a>]</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.11687" style="box-sizing: border-box; color: rgb(3, 102, 214);">Extreme Language Model Compression with Optimal Subwords and Shared Projections</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.05840" style="box-sizing: border-box; color: rgb(3, 102, 214);">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</a></div></li></ul><h2 style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-weight: bold; font-size: 1.5em; line-height: 1.25; padding-bottom: 0.3em; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><span style="box-sizing: border-box; font-weight: bold; font-size: 1.5em; line-height: 1.25; border-bottom: 1px solid rgb(234, 236, 239); color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">Misc.</span></h2><ul style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 0px !important; color: rgb(36, 41, 46); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="box-sizing: border-box;"><div><a href="https://arxiv.org/abs/1903.07785" style="box-sizing: border-box; color: rgb(3, 102, 214);">Cloze-driven Pretraining of Self-attention Networks</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1901.11373" style="box-sizing: border-box; color: rgb(3, 102, 214);">Learning and Evaluating General Linguistic Intelligence</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1903.05987" style="box-sizing: border-box; color: rgb(3, 102, 214);">To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</a> (ACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.09675" style="box-sizing: border-box; color: rgb(3, 102, 214);">BERTScore: Evaluating Text Generation with BERT</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.12679" style="box-sizing: border-box; color: rgb(3, 102, 214);">Machine Translation Evaluation with BERT Regressor</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1904.00962" style="box-sizing: border-box; color: rgb(3, 102, 214);">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1907.11932" style="box-sizing: border-box; color: rgb(3, 102, 214);">Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1901.10125" style="box-sizing: border-box; color: rgb(3, 102, 214);">Glyce: Glyph-vectors for Chinese Character Representations</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://arxiv.org/abs/1909.03464" style="box-sizing: border-box; color: rgb(3, 102, 214);">Back to the Future -- Sequential Alignment of Text Representations</a></div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-1402/" style="box-sizing: border-box; color: rgb(3, 102, 214);">Improving Cuneiform Language Identification with BERT</a> (NAACL2019 WS)</div></li><li style="box-sizing: border-box; margin-top: 0.25em;"><div><a href="https://dl.acm.org/citation.cfm?id=3342186" style="box-sizing: border-box; color: rgb(3, 102, 214);">SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction</a> (ACM-BCB2019)</div></li></ul><div style="margin: 0px; padding: 0px; clear: both; min-height: 1em; max-width: 100% !important; box-sizing: border-box !important; word-wrap: break-word !important;"><br/></div></div></div></div></div></div><div><br/></div></span>
</div></body></html> 